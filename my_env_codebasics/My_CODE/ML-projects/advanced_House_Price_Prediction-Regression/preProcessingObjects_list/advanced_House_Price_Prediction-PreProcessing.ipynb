{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f3912de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77060607",
   "metadata": {},
   "source": [
    "# Import features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ffaba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import training features\n",
    "import pathlib\n",
    "path_to_read_model = 'C:\\\\Users\\\\koriv\\\\Desktop\\\\MachineLearning_DataScience\\\\Hands_On_Machine_Learning\\\\my_env_codebasics\\\\My_CODE\\\\ML-projects\\\\advanced_House_Price_Prediction-Regression\\\\probleam_study' #Path of current working Directory\n",
    "with open(path_to_read_model + '\\\\trainFeatures_list.pkl', 'rb') as f:\n",
    "    X, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1994690",
   "metadata": {},
   "source": [
    "# PreProcessing Objects needed for Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb845f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Whitefield', 'Sarjapur  Road', 'Electronic City', 'Kanakpura Road',\n",
       "       'Thanisandra', 'Hebbal', 'Marathahalli', 'Uttarahalli', 'Yelahanka',\n",
       "       'Raja Rajeshwari Nagar',\n",
       "       ...\n",
       "       'Kalyan nagar', 'Battarahalli', 'Kothannur', 'Kambipura',\n",
       "       'Kumaraswami Layout', 'Anjanapura', 'Banaswadi', 'Hosakerehalli',\n",
       "       'HRBR Layout', 'BTM 2nd Stage'],\n",
       "      dtype='object', length=117)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_stats_greater_than_10 = (X['location'].apply(lambda x: str(x).strip()).value_counts(ascending=False, dropna=False)[X['location'].apply(lambda x: str(x).strip()).value_counts(ascending=False, dropna=False)>10]).index.unique()\n",
    "location_stats_greater_than_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd8bccb",
   "metadata": {},
   "source": [
    "# Saving the PreProcessing Objects Using Pickel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e690af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preProcessingObjects_list = [location_stats_greater_than_10]\n",
    "\n",
    "import pathlib\n",
    "path_to_write_output=str(pathlib.Path.cwd()) #Path of current working Directory\n",
    "with open(path_to_write_output + '\\\\preProcessingObjects_list.pkl', 'wb') as handle:\n",
    "    pickle.dump(preProcessingObjects_list, handle)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "979d8b51",
   "metadata": {},
   "source": [
    "########## *************************** #####################  \n",
    "########## *************************** #####################\n",
    "########## *************************** #####################\n",
    "########## *************************** #####################\n",
    "########## *************************** #####################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86cc85",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1637760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transformations(featureDF, targetDF):\n",
    "    # Load the PreProcessing Objects needed for Transformations\n",
    "    #Import training features\n",
    "    import pathlib\n",
    "    path_to_read_model = 'C:\\\\Users\\\\koriv\\\\Desktop\\\\MachineLearning_DataScience\\\\Hands_On_Machine_Learning\\\\my_env_codebasics\\\\My_CODE\\\\ML-projects\\\\advanced_House_Price_Prediction-Regression\\\\preProcessingObjects_list' #Path of current working Directory\n",
    "    with open(path_to_read_model + '\\\\preProcessingObjects_list.pkl', 'rb') as f:\n",
    "        preProcessingObjects_list = pickle.load(f)\n",
    "    location_stats_greater_than_10 = preProcessingObjects_list[0]\n",
    "    \n",
    "    \n",
    "    # Feature Selection via Business knoledge. Society is dependednt on location\n",
    "    featureDF.drop(['society'], axis=1, inplace=True)\n",
    "    \n",
    "    # Duplicate elimination\n",
    "    featureDF.drop_duplicates(inplace=True)\n",
    "    # Update y matrix based X\n",
    "    ## since we've removed some data from X, we need to pass on these updations to y as well, as y doesn't know some of its corresponding X's have been deleted.\n",
    "    targetDF = targetDF[featureDF.index]\n",
    "    \n",
    "    # convert cat-col of total_sqft to float with null values also\n",
    "    def convert_sqft_to_num(curr_tuple):\n",
    "        try:\n",
    "            tokens = curr_tuple.split('-')\n",
    "            if len(tokens) == 2:\n",
    "                return (float(tokens[0])+float(tokens[1]))/2\n",
    "            return float(curr_tuple)\n",
    "        except:\n",
    "            return np.NaN\n",
    "    \n",
    "    featureDF['total_sqft'] = featureDF.total_sqft.apply(convert_sqft_to_num)\n",
    "    \n",
    "    # convert cat-col of availability to make both Immediate Possession and Ready To Move same and take only month, include null also.\n",
    "    def convert_availability(curr_tuple):\n",
    "        try:\n",
    "            curr_tuple = curr_tuple.lower()\n",
    "            if curr_tuple == 'ready to move' or curr_tuple == 'immediate possession':\n",
    "                return 'available_currently'\n",
    "            tokens = curr_tuple.split('-')\n",
    "            if len(tokens) == 2:\n",
    "                return tokens[1].strip()\n",
    "        except:\n",
    "            return np.NaN\n",
    "        \n",
    "    featureDF['availability'] = featureDF.availability.apply(convert_availability)\n",
    "    \n",
    "    # convert cat-col of size to number by the frist value with null value also\n",
    "    def convert_size_to_num(curr_tuple):\n",
    "        try:\n",
    "            tokens = curr_tuple.split(' ')\n",
    "            return float(tokens[0])\n",
    "        except:\n",
    "            return np.NaN\n",
    "        \n",
    "    featureDF['size'] = featureDF['size'].apply(convert_size_to_num)\n",
    "    \n",
    "    # Convert location to reduce the unique values in the column\n",
    "    X['location'] = X['location'].apply(lambda x: x if (x in location_stats_greater_than_10) else 'other')\n",
    "    \n",
    "    # Update y matrix based X\n",
    "    ## since we've removed some data from X, we need to pass on these updations to y as well, as y doesn't know some of its corresponding X's have been deleted.\n",
    "    targetDF = targetDF[featureDF.index]\n",
    "    \n",
    "    return featureDF, targetDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea7934",
   "metadata": {},
   "source": [
    "# Saving the Transformation function Using Pickel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06310bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "path_to_write_output=str(pathlib.Path.cwd()) #Path of current working Directory\n",
    "with open(path_to_write_output + '\\\\transformation_function.pkl', 'wb') as handle:\n",
    "    pickle.dump(Transformations, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2103371d",
   "metadata": {},
   "source": [
    "# Transforming features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "970d840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PreProcessing Objects needed for Transformations\n",
    "#Import training features\n",
    "import pathlib\n",
    "path_to_read_model = 'C:\\\\Users\\\\koriv\\\\Desktop\\\\MachineLearning_DataScience\\\\Hands_On_Machine_Learning\\\\my_env_codebasics\\\\My_CODE\\\\ML-projects\\\\advanced_House_Price_Prediction-Regression\\\\preProcessingObjects_list' #Path of current working Directory\n",
    "with open(path_to_read_model + '\\\\transformation_function.pkl', 'rb') as f:\n",
    "    transformation_function = pickle.load(f)\n",
    "\n",
    "X, y = transformation_function(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "137cde42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in the Training feature: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "area_type         0\n",
       "availability      0\n",
       "location          0\n",
       "size              4\n",
       "total_sqft       15\n",
       "bath             25\n",
       "balcony         264\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "Null values in the Training feature: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "Shape of Training Features:  (4928, 7)\n",
      "Shape of Training target:  (4928,)\n"
     ]
    }
   ],
   "source": [
    "#Check for null values in the Train and Test Data\n",
    "print(\"Null values in the Training feature: \")\n",
    "display(X.isnull().sum())\n",
    "#print(\"Null values in the Test feature: \")\n",
    "#display(X_test.isnull().sum())\n",
    "\n",
    "print(\"-\"*124)\n",
    "\n",
    "#Check for null values in the Target\n",
    "print(\"Null values in the Training feature: \")\n",
    "display(y.isnull().sum())\n",
    "#print(\"Null values in the Test feature: \")\n",
    "#display(y_test.isnull().sum())\n",
    "\n",
    "print(\"-\"*124)\n",
    "\n",
    "#Check for DataSets Shape\n",
    "print(\"Shape of Training Features: \",X.shape)\n",
    "print(\"Shape of Training target: \",y.shape)\n",
    "#print(\"Shape of Testing Features: \",X_test.shape)\n",
    "#print(\"Shape of Testing target: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "784cfd76",
   "metadata": {},
   "source": [
    "########## *************************** #####################  \n",
    "########## *************************** #####################\n",
    "########## *************************** #####################\n",
    "########## *************************** #####################\n",
    "########## *************************** #####################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcde7bb",
   "metadata": {},
   "source": [
    "# Outlier Specifications based on IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07bbec25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Numerical Columns in the Data: \n",
      " ['size', 'total_sqft', 'bath', 'balcony']\n",
      ">>Catogorical Columns are: \n",
      " ['area_type', 'availability', 'location']\n"
     ]
    }
   ],
   "source": [
    "num_cols = [col for col in X.columns if X[col].dtypes!='O']\n",
    "print(\">>Numerical Columns in the Data: \\n\",num_cols)\n",
    "cat_cols = [col for col in X if X[col].dtypes == 'O']\n",
    "print(\">>Catogorical Columns are: \\n\",cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04a0376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>numerical columns in analysis:  ['size', 'total_sqft', 'bath', 'balcony']\n",
      ">>Outlier IQR results:\n",
      " {\n",
      "    \"size\": {\n",
      "        \"25quantile\": 2.0,\n",
      "        \"75quantile\": 3.0,\n",
      "        \"iqr\": 1.0,\n",
      "        \"min\": 1.0,\n",
      "        \"max\": 27.0,\n",
      "        \"median\": 3.0,\n",
      "        \"lower_limit\": 0.5,\n",
      "        \"upper_limit\": 4.5\n",
      "    },\n",
      "    \"total_sqft\": {\n",
      "        \"25quantile\": 1100.0,\n",
      "        \"75quantile\": 1690.0,\n",
      "        \"iqr\": 590.0,\n",
      "        \"min\": 1.0,\n",
      "        \"max\": 52272.0,\n",
      "        \"median\": 1296.0,\n",
      "        \"lower_limit\": 215.0,\n",
      "        \"upper_limit\": 2575.0\n",
      "    },\n",
      "    \"bath\": {\n",
      "        \"25quantile\": 2.0,\n",
      "        \"75quantile\": 3.0,\n",
      "        \"iqr\": 1.0,\n",
      "        \"min\": 1.0,\n",
      "        \"max\": 27.0,\n",
      "        \"median\": 2.0,\n",
      "        \"lower_limit\": 0.5,\n",
      "        \"upper_limit\": 4.5\n",
      "    },\n",
      "    \"balcony\": {\n",
      "        \"25quantile\": 1.0,\n",
      "        \"75quantile\": 2.0,\n",
      "        \"iqr\": 1.0,\n",
      "        \"min\": 0.0,\n",
      "        \"max\": 3.0,\n",
      "        \"median\": 2.0,\n",
      "        \"lower_limit\": -0.5,\n",
      "        \"upper_limit\": 3.5\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\">>numerical columns in analysis: \", num_cols)\n",
    "outlier_Dict = dict()\n",
    "for col in num_cols:\n",
    "    X_min = X[col].min()\n",
    "    X_max = X[col].max()\n",
    "    median = X[col].median()\n",
    "    q1 = X[col].quantile(0.25)\n",
    "    q3 = X[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    upper_limit = q3 + 1.5 * iqr\n",
    "    lower_limit = q1 - 1.5 * iqr\n",
    "    outlier_Dict[col] = {'25quantile':q1, '75quantile':q3, \n",
    "                         'iqr':iqr, 'min': X_min, 'max':X_max, 'median':median, \n",
    "                         'lower_limit':lower_limit, 'upper_limit':upper_limit}\n",
    "    \n",
    "#Check the Outlier Specifics\n",
    "import json\n",
    "print(\">>Outlier IQR results:\\n\",json.dumps(outlier_Dict,sort_keys=False, indent=4, default=str))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6434470e",
   "metadata": {},
   "source": [
    "# Save the Outlier Specification to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3454064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outlier_Dict.json', 'w') as fp:\n",
    "    json.dump(outlier_Dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5628db8",
   "metadata": {},
   "source": [
    "# Outlier Removal functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc5c2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlierRemoval(featureDF, targetDF):\n",
    "    outlier_Dict_DirPath = 'C:\\\\Users\\\\koriv\\\\Desktop\\\\MachineLearning_DataScience\\\\Hands_On_Machine_Learning\\\\my_env_codebasics\\\\My_CODE\\\\ML-projects\\\\advanced_House_Price_Prediction-Regression\\\\preProcessingObjects_list'\n",
    "    with open(outlier_Dict_DirPath+'\\\\outlier_Dict.json') as json_file:\n",
    "        outlier_Dict = json.load(json_file)\n",
    "    \n",
    "    for column in outlier_Dict:\n",
    "        lower_limit =outlier_Dict[column]['lower_limit']\n",
    "        upper_limit =outlier_Dict[column]['upper_limit']\n",
    "        print(\"column: %s, lower_limit: %s, upper_limit: %s\"%(column, lower_limit, upper_limit))\n",
    "        featureDF = featureDF[((featureDF[column]>lower_limit)&(featureDF[column]<upper_limit))| (featureDF[column].isna())]\n",
    "    \n",
    "    # Update y matrix based X\n",
    "    ## since we've removed some data from X, we need to pass on these updations to y as well, as y doesn't know some of its corresponding X's have been deleted.\n",
    "    targetDF = targetDF[featureDF.index]\n",
    "    \n",
    "    return featureDF, targetDF    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbae161",
   "metadata": {},
   "source": [
    "# Saving the OutlierRemoval function Using Pickel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14e1bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "path_to_write_output=str(pathlib.Path.cwd()) #Path of current working Directory\n",
    "with open(path_to_write_output + '\\\\outlierRemoval_function.pkl', 'wb') as handle:\n",
    "    pickle.dump(outlierRemoval, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e507f",
   "metadata": {},
   "source": [
    "# Handeling Outlier of features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b48d613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column: size, lower_limit: 0.5, upper_limit: 4.5\n",
      "column: total_sqft, lower_limit: 215.0, upper_limit: 2575.0\n",
      "column: bath, lower_limit: 0.5, upper_limit: 4.5\n",
      "column: balcony, lower_limit: -0.5, upper_limit: 3.5\n"
     ]
    }
   ],
   "source": [
    "# Load the PreProcessing Objects needed for Transformations\n",
    "#Import training features\n",
    "import pathlib\n",
    "path_to_read_model = 'C:\\\\Users\\\\koriv\\\\Desktop\\\\MachineLearning_DataScience\\\\Hands_On_Machine_Learning\\\\my_env_codebasics\\\\My_CODE\\\\ML-projects\\\\advanced_House_Price_Prediction-Regression\\\\preProcessingObjects_list' #Path of current working Directory\n",
    "with open(path_to_read_model + '\\\\outlierRemoval_function.pkl', 'rb') as f:\n",
    "    outlierRemoval_function = pickle.load(f)\n",
    "\n",
    "X, y = outlierRemoval_function(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bffcbd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in the Training feature: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "area_type         0\n",
       "availability      0\n",
       "location          0\n",
       "size              3\n",
       "total_sqft       12\n",
       "bath             16\n",
       "balcony         124\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "Null values in the Training feature: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "Shape of Training Features:  (4225, 7)\n",
      "Shape of Training target:  (4225,)\n"
     ]
    }
   ],
   "source": [
    "#Check for null values in the Train and Test Data\n",
    "print(\"Null values in the Training feature: \")\n",
    "display(X.isnull().sum())\n",
    "\n",
    "\n",
    "print(\"-\"*124)\n",
    "\n",
    "#Check for null values in the Target\n",
    "print(\"Null values in the Training feature: \")\n",
    "display(y.isnull().sum())\n",
    "\n",
    "\n",
    "print(\"-\"*124)\n",
    "\n",
    "#Check for DataSets Shape\n",
    "print(\"Shape of Training Features: \",X.shape)\n",
    "print(\"Shape of Training target: \",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c1a0edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>total_sqft</th>\n",
       "      <th>bath</th>\n",
       "      <th>balcony</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4222.000000</td>\n",
       "      <td>4213.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.485315</td>\n",
       "      <td>1324.157277</td>\n",
       "      <td>2.337372</td>\n",
       "      <td>1.567910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.704100</td>\n",
       "      <td>409.554657</td>\n",
       "      <td>0.702882</td>\n",
       "      <td>0.792393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1088.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1250.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1548.300000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2572.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              size   total_sqft         bath      balcony\n",
       "count  4222.000000  4213.000000  4209.000000  4101.000000\n",
       "mean      2.485315  1324.157277     2.337372     1.567910\n",
       "std       0.704100   409.554657     0.702882     0.792393\n",
       "min       1.000000   250.000000     1.000000     0.000000\n",
       "25%       2.000000  1088.000000     2.000000     1.000000\n",
       "50%       2.000000  1250.000000     2.000000     2.000000\n",
       "75%       3.000000  1548.300000     3.000000     2.000000\n",
       "max       4.000000  2572.000000     4.000000     3.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f5f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f864842e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec09245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58061a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
